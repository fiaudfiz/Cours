üìù Module 4 : La Hi√©rarchie M√©moire (Memory Hierarchy)
1. Le Probl√®me : Le "Memory Wall"

Le processeur (CPU) va des centaines de fois plus vite que la m√©moire vive (RAM). Pour ne pas que le CPU passe sa vie √† attendre ses donn√©es, on utilise une structure en pyramide.
2. La Pyramide des Caches (Du plus rapide au plus lent)
Niveau	Emplacement	Capacit√© (env.)	Latence (cycles)
Registres	Dans le c≈ìur CPU	~1 Ko	0 cycle
Cache L1	Coll√© au c≈ìur	32 - 64 Ko	~4 cycles
Cache L2	Proche du c≈ìur	256 - 512 Ko	~12 cycles
Cache L3	Partag√© (tous c≈ìurs)	8 - 64 Mo	~40 cycles
RAM	Barrette externe	16 - 64 Go	~200+ cycles
Shutterstock
3. M√©canisme : Cache Hit vs Cache Miss

Le CPU ne parle jamais directement √† la RAM s'il peut l'√©viter.

    Cache Hit (Succ√®s) : La donn√©e est trouv√©e dans l'un des caches. Le CPU continue √† pleine vitesse.

    Cache Miss (√âchec) : La donn√©e n'est nulle part dans les caches. Le CPU doit "caler" (Stall) et attendre que la donn√©e remonte de la RAM.

4. Le Principe de Localit√© (Comment le cache anticipe)

Le succ√®s d'un programme repose sur deux comportements que le cache exploite :
A. Localit√© Temporelle (Le "D√©j√†-vu")

    Concept : Si une donn√©e est utilis√©e, elle a de fortes chances d'√™tre r√©utilis√©e tr√®s bient√¥t.

    Application : Les compteurs de boucles (i, j), les accumulateurs de somme.

    Gestion : Le cache garde la donn√©e au chaud. Si le cache est plein, il utilise l'algorithme LRU (Least Recently Used) : il √©vince la donn√©e la plus ancienne pour faire de la place.

B. Localit√© Spatiale (Le "Voisinage" / Cache Line)

    Concept : Si on acc√®de √† une adresse X, on acc√©dera probablement √† X+1,X+2...

    L'Unit√© de transfert : La Cache Line. Le CPU ne d√©place jamais moins de 64 octets √† la fois entre la RAM et le cache.

    Exemple : Si tu demandes un int (4 octets) √† l'index tab[0], le CPU charge en r√©alit√© les 64 octets suivants. Tu re√ßois donc tab[0] √† tab[15] dans ton cache L1 instantan√©ment. Les 15 suivants sont "gratuits".

5. Cas d'√©cole : Le parcours de Matrice

Une matrice int matrix[10000][10000] est stock√©e de mani√®re contigu√´ (en un seul bloc) en RAM, ligne par ligne (Row-major order).

    Parcours par Ligne (Rapide) : En lisant matrix[0][0] puis matrix[0][1], tu profites de la Cache Line. Tu as 1 Cache Miss pour 16 acc√®s.

    Parcours par Colonne (Lent) : En lisant matrix[0][0] puis matrix[1][0], tu sautes de 40 000 octets en RAM √† chaque fois. Tu tombes syst√©matiquement en dehors de la Cache Line actuelle.

    R√©sultat : 100% de Cache Miss. Le code peut √™tre 10 √† 50 fois plus lent.